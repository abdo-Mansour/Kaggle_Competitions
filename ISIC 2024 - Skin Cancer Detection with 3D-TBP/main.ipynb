{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63056,"databundleVersionId":8940774,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:01:59.192343Z","iopub.execute_input":"2024-07-09T21:01:59.192846Z","iopub.status.idle":"2024-07-09T21:02:13.496467Z","shell.execute_reply.started":"2024-07-09T21:01:59.192802Z","shell.execute_reply":"2024-07-09T21:02:13.495322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.torch_version\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchsummary import summary\nfrom torchvision import transforms\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:13.498676Z","iopub.execute_input":"2024-07-09T21:02:13.498994Z","iopub.status.idle":"2024-07-09T21:02:15.951734Z","shell.execute_reply.started":"2024-07-09T21:02:13.498965Z","shell.execute_reply":"2024-07-09T21:02:15.950872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:15.952966Z","iopub.execute_input":"2024-07-09T21:02:15.953240Z","iopub.status.idle":"2024-07-09T21:02:16.019712Z","shell.execute_reply.started":"2024-07-09T21:02:15.953217Z","shell.execute_reply":"2024-07-09T21:02:16.018645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nmain_path = '/kaggle/input/isic-2024-challenge' if iskaggle else 'data/isic-2024-challenge'","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:16.022588Z","iopub.execute_input":"2024-07-09T21:02:16.023091Z","iopub.status.idle":"2024-07-09T21:02:16.027959Z","shell.execute_reply.started":"2024-07-09T21:02:16.023056Z","shell.execute_reply":"2024-07-09T21:02:16.026990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_metadata_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv' if iskaggle else 'data/isic-2024-challenge/train-metadata.csv'\ntest_metadata_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv' if iskaggle else 'data/isic-2024-challenge/test-metadata.csv'\n\ntrain_metadata_df = pd.read_csv(train_metadata_path)\ntest_metadata_df = pd.read_csv(test_metadata_path)\n\nprint(len(train_metadata_df))","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:16.028978Z","iopub.execute_input":"2024-07-09T21:02:16.029283Z","iopub.status.idle":"2024-07-09T21:02:23.763596Z","shell.execute_reply.started":"2024-07-09T21:02:16.029258Z","shell.execute_reply":"2024-07-09T21:02:23.762545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.model_selection as train_test_split\n\ntrain_size = 0.8\n# Splitting the train dataset into positive and negative samples and saving them in separate dataframes\npostive_samples = train_metadata_df[train_metadata_df['target'] == 1]\nnegative_samples = train_metadata_df[train_metadata_df['target'] == 0]\nprint(f\"Positive samples: {postive_samples.shape}\")\nprint(f\"Negative samples: {negative_samples.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:23.764734Z","iopub.execute_input":"2024-07-09T21:02:23.765038Z","iopub.status.idle":"2024-07-09T21:02:24.552941Z","shell.execute_reply.started":"2024-07-09T21:02:23.765011Z","shell.execute_reply":"2024-07-09T21:02:24.551778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Splitting each type of samples into train and validation sets\ntrain_positive_samples, val_positive_samples = train_test_split.train_test_split(postive_samples,test_size=1-train_size)\ntrain_negative_samples, val_negative_samples = train_test_split.train_test_split(negative_samples,test_size=1-train_size)\nprint(f\"Train positive samples: {train_positive_samples.shape}\")\nprint(f\"Train negative samples: {train_negative_samples.shape}\")\nprint(f\"Val positive samples: {val_positive_samples.shape}\")\nprint(f\"Val negative samples: {val_negative_samples.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:24.554151Z","iopub.execute_input":"2024-07-09T21:02:24.554399Z","iopub.status.idle":"2024-07-09T21:02:25.069896Z","shell.execute_reply.started":"2024-07-09T21:02:24.554377Z","shell.execute_reply":"2024-07-09T21:02:25.068906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Concatenating the positive and negative samples to get the train and validation sets\ntrain_metadata_df = pd.concat([train_positive_samples, train_negative_samples])\nval_metadata_df = pd.concat([val_positive_samples, val_negative_samples])\nprint(f\"Train samples: {train_metadata_df.shape}\")\nprint(f\"Val samples: {val_metadata_df.shape}\")\nprint(f\"Test samples: {test_metadata_df.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.071311Z","iopub.execute_input":"2024-07-09T21:02:25.072591Z","iopub.status.idle":"2024-07-09T21:02:25.243947Z","shell.execute_reply.started":"2024-07-09T21:02:25.072548Z","shell.execute_reply":"2024-07-09T21:02:25.242908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\nfrom io import BytesIO\n\ntrain_hdf5_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5' if iskaggle else 'data/isic-2024-challenge/train-image.hdf5'\ntest_hdf5_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5' if iskaggle else 'data/isic-2024-challenge/test-image.hdf5'","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.245020Z","iopub.execute_input":"2024-07-09T21:02:25.245287Z","iopub.status.idle":"2024-07-09T21:02:25.398204Z","shell.execute_reply.started":"2024-07-09T21:02:25.245264Z","shell.execute_reply":"2024-07-09T21:02:25.397429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_hdf5_file = h5py.File(train_hdf5_path,'r')\ntest_hdf5_file = h5py.File(test_hdf5_path,'r')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.401104Z","iopub.execute_input":"2024-07-09T21:02:25.401415Z","iopub.status.idle":"2024-07-09T21:02:25.415008Z","shell.execute_reply.started":"2024-07-09T21:02:25.401388Z","shell.execute_reply":"2024-07-09T21:02:25.414151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, hdf5_file, metadata_df,target=None, transform=None):\n        self.hdf5_file = hdf5_file\n        self.metadata_df = metadata_df\n        self.image_ids = metadata_df['isic_id']\n        self.labels = target\n        self.transform = transform\n\n        self.mean_of_color_channels = None  # Initialize as None\n        self.std_of_color_channels = None   # Initialize as None\n        # self._calculate_stats()\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids.iloc[idx]\n        image = Image.open(BytesIO(self.hdf5_file[image_id][()]))\n\n        # Calculate mean and standard deviation if not already done\n        # if self.mean_of_color_channels is None or self.std_of_color_channels is None:\n        #     self._calculate_stats()\n\n        # Apply transformation with calculated statistics\n        if self.transform:\n            image = self.transform(image)\n\n        if self.labels is not None:\n            label = self.labels.iloc[idx]\n            return image, label\n        else:\n            return image\n   ","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.416362Z","iopub.execute_input":"2024-07-09T21:02:25.416693Z","iopub.status.idle":"2024-07-09T21:02:25.425960Z","shell.execute_reply.started":"2024-07-09T21:02:25.416667Z","shell.execute_reply":"2024-07-09T21:02:25.424947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# HyperParameters\ndim = 50\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.427144Z","iopub.execute_input":"2024-07-09T21:02:25.427427Z","iopub.status.idle":"2024-07-09T21:02:25.438817Z","shell.execute_reply.started":"2024-07-09T21:02:25.427390Z","shell.execute_reply":"2024-07-09T21:02:25.437636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([ \n    transforms.Resize((dim, dim)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    # transforms.Normalize(mean_of_color_channels, std_of_color_channels),\n])\n\ntrain_transform_modified = transforms.Compose([ \n    transforms.Resize((dim, dim)),\n    transforms.RandomRotation(20),\n    transforms.RandomHorizontalFlip(1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    # transforms.Normalize(mean_of_color_channels, std_of_color_channels),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((dim, dim)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    # transforms.Normalize(mean_of_color_channels, std_of_color_channels),\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.440053Z","iopub.execute_input":"2024-07-09T21:02:25.440770Z","iopub.status.idle":"2024-07-09T21:02:25.449524Z","shell.execute_reply.started":"2024-07-09T21:02:25.440716Z","shell.execute_reply":"2024-07-09T21:02:25.448381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_hdf5_file,train_metadata_df,target=train_metadata_df['target'],transform=train_transform)\nval_dataset = CustomDataset(train_hdf5_file,val_metadata_df,target=val_metadata_df['target'],transform=train_transform)\ntest_dataset = CustomDataset(test_hdf5_file,test_metadata_df,transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.451485Z","iopub.execute_input":"2024-07-09T21:02:25.452096Z","iopub.status.idle":"2024-07-09T21:02:25.462600Z","shell.execute_reply.started":"2024-07-09T21:02:25.452063Z","shell.execute_reply":"2024-07-09T21:02:25.461663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\nval_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True)\ntest_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.463530Z","iopub.execute_input":"2024-07-09T21:02:25.463837Z","iopub.status.idle":"2024-07-09T21:02:25.472931Z","shell.execute_reply.started":"2024-07-09T21:02:25.463813Z","shell.execute_reply":"2024-07-09T21:02:25.471995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,num_classes,device,dim = 32,num_epochs = 20,learning_rate = 0.001,early_stopping = False):\n        super().__init__()\n        self.num_of_classes = num_classes\n        self.device = device\n        self.dim = dim\n        # Debugging\n        self.DEBUG = True\n        # Hyperparameters\n        self.num_epochs = num_epochs\n        self.learning_rate = learning_rate\n        self.early_stopping = early_stopping\n        # History while Training\n        self.model_loss_history = []\n        self.model_train_acc_history = []\n        self.model_val_acc_history = []\n        self.model_val_precision_history = []\n        self.model_val_recall_history = []\n        self.model_lr_history = []\n\n        # Model Attributes\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = None\n        self.accuracy = Accuracy(task= 'multiclass', num_classes=self.num_of_classes, average='macro').to(self.device)\n        self.precision = Precision(task= 'multiclass', num_classes=self.num_of_classes, average='macro').to(self.device)\n        self.recall = Recall(task= 'multiclass', num_classes=self.num_of_classes, average='macro').to(self.device)\n        # Model Architecture\n        self.feature_extract = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Flatten(),\n        )\n        self.classifier = nn.Sequential(            \n            nn.Linear(20000, self.num_of_classes),\n        )\n        \n    def forward(self, x):\n        x = self.feature_extract(x)\n        x = self.classifier(x)\n        return x\n    \n    def predict(self, img):\n        '''\n        returns the predicted classes for the given images\n        '''\n        self.eval()\n        with torch.no_grad():\n            img = img.to(self.device)\n            output = self(img)\n            _, predicted = torch.max(output, 1)\n            return predicted\n        \n\n    \n    def eval_val(self, data_loader):\n        '''\n        returns accuracy, precision and recall\n        '''\n        self.eval()\n        with torch.no_grad():\n            for images, labels in data_loader:\n                \n                images, labels = images.to(self.device), labels.to(self.device)\n                outputs = self(images)\n                self.accuracy(outputs, labels)\n                self.precision(outputs, labels)\n                self.recall(outputs, labels)\n\n        return self.accuracy.compute(), self.precision.compute(), self.recall.compute()\n    \n    def train_model(self, train_loader, val_loader):\n        \n        last_accuracy = -100\n        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n\n        for epoch in range(self.num_epochs):\n            self.train()\n            running_loss = 0.0\n\n            for i, (images, labels) in enumerate(train_loader):\n\n                images, labels = images.to(self.device), labels.to(self.device)\n                self.optimizer.zero_grad()\n                outputs = self(images)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss += loss.item()\n                if i%1000 == 0 and self.DEBUG:\n                    print(\" Step [{}/{}] Loss: {}\".format(i, len(train_loader), loss.item()))\n                    \n            val_acc, val_precision, val_recall = self.eval_val(val_loader)\n            train_acc, _, _ = self.eval_val(train_loader)\n\n            self.model_loss_history.append(running_loss/len(train_loader))\n            self.model_train_acc_history.append(train_acc.item())\n            self.model_val_acc_history.append(val_acc.item())\n            self.model_val_precision_history.append(val_precision.item())\n            self.model_val_recall_history.append(val_recall.item())\n            self.model_lr_history.append(self.optimizer.param_groups[0]['lr'])\n            \n            print(f'Epoch: {epoch+1}/{self.num_epochs}, Loss: {loss.item()},Train Acc: {train_acc}, Val Acc: {val_acc}, Val Precision: {val_precision}, Val Recall: {val_recall}')\n            \n            if val_acc > last_accuracy:\n                last_accuracy = val_acc\n            elif self.early_stopping:\n                break\n        \n        print('Finished Training')\n\n    def plot_history(self):\n        # making two plots one for loss and other for accuracy\n        fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n        fig.suptitle('Model Training History')\n        axs[0, 0].plot(self.model_loss_history)\n        axs[0, 0].set_title('Model Loss')\n        axs[0, 0].set_xlabel('Epochs')\n        axs[0, 0].set_ylabel('Loss')\n\n        axs[0, 1].plot(self.model_train_acc_history, label='Train')\n        axs[0, 1].plot(self.model_val_acc_history, label='Val')\n        axs[0, 1].set_title('Model Accuracy')\n        axs[0, 1].set_xlabel('Epochs')\n        axs[0, 1].set_ylabel('Accuracy')\n        axs[0, 1].legend()\n\n        axs[1, 0].plot(self.model_val_precision_history)\n        axs[1, 0].set_title('Model Precision')\n        axs[1, 0].set_xlabel('Epochs')\n        axs[1, 0].set_ylabel('Precision')\n        \n        axs[1, 1].plot(self.model_val_recall_history)\n        axs[1, 1].set_title('Model Recall')\n        axs[1, 1].set_xlabel('Epochs')\n        axs[1, 1].set_ylabel('Recall')\n\n        axs[0, 2].plot(self.model_lr_history)\n        axs[0, 2].set_title('Learning Rate')\n        axs[0, 2].set_xlabel('Epochs')\n        axs[0, 2].set_ylabel('Learning Rate')\n        \n        \n        # axs[1, 2].axis('off')\n\n        plt.show()\n    \n    def save_model(self):\n        torch.save(self.state_dict(),type(self).__name__+'.pth')\n\n    def print_summary(self):\n        summary(self, (3, self.dim, self.dim))","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.474266Z","iopub.execute_input":"2024-07-09T21:02:25.474555Z","iopub.status.idle":"2024-07-09T21:02:25.506619Z","shell.execute_reply.started":"2024-07-09T21:02:25.474528Z","shell.execute_reply":"2024-07-09T21:02:25.505740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_classes = 2\ncnn = Model(num_classes=num_of_classes, \n            device=device, \n            dim=dim, \n            num_epochs=5, \n            learning_rate=0.001,\n            early_stopping=False)\ncnn.to(device)\ncnn.print_summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:25.507899Z","iopub.execute_input":"2024-07-09T21:02:25.508194Z","iopub.status.idle":"2024-07-09T21:02:26.301121Z","shell.execute_reply.started":"2024-07-09T21:02:25.508170Z","shell.execute_reply":"2024-07-09T21:02:26.300015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn.train_model(train_loader=train_loader,val_loader=val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T21:02:26.302525Z","iopub.execute_input":"2024-07-09T21:02:26.302899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_acc , cnn_precision, cnn_recall = cnn.eval_val(val_loader)\nprint(f\"Accuracy: {cnn_acc}, Precision: {cnn_precision}, Recall: {cnn_recall}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn.plot_history()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}