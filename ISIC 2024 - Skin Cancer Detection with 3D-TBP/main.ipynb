{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:00:21.092904Z","iopub.status.busy":"2024-07-16T13:00:21.092050Z","iopub.status.idle":"2024-07-16T13:00:34.756132Z","shell.execute_reply":"2024-07-16T13:00:34.754942Z","shell.execute_reply.started":"2024-07-16T13:00:21.092858Z"},"trusted":true},"outputs":[],"source":["# !pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:30:55.994560Z","iopub.status.busy":"2024-07-16T13:30:55.994103Z","iopub.status.idle":"2024-07-16T13:30:56.003413Z","shell.execute_reply":"2024-07-16T13:30:56.002273Z","shell.execute_reply.started":"2024-07-16T13:30:55.994526Z"},"trusted":true},"outputs":[],"source":["import torch\n","import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.torch_version\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import DataLoader, Dataset\n","from torchsummary import summary\n","from torchvision import transforms\n","from torchmetrics import Accuracy, Precision, Recall\n","\n","import cv2\n","from joblib import Parallel, delayed\n","\n","from torch.cuda.amp import GradScaler, autocast\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:30:56.271759Z","iopub.status.busy":"2024-07-16T13:30:56.270966Z","iopub.status.idle":"2024-07-16T13:30:56.277940Z","shell.execute_reply":"2024-07-16T13:30:56.276696Z","shell.execute_reply.started":"2024-07-16T13:30:56.271723Z"},"trusted":true},"outputs":[],"source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:30:57.111423Z","iopub.status.busy":"2024-07-16T13:30:57.110976Z","iopub.status.idle":"2024-07-16T13:30:57.116489Z","shell.execute_reply":"2024-07-16T13:30:57.115325Z","shell.execute_reply.started":"2024-07-16T13:30:57.111389Z"},"trusted":true},"outputs":[],"source":["iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n","main_path = '/kaggle/input/isic-2024-challenge' if iskaggle else 'data/isic-2024-challenge'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:30:57.682463Z","iopub.status.busy":"2024-07-16T13:30:57.681681Z","iopub.status.idle":"2024-07-16T13:31:02.954530Z","shell.execute_reply":"2024-07-16T13:31:02.953271Z","shell.execute_reply.started":"2024-07-16T13:30:57.682428Z"},"trusted":true},"outputs":[],"source":["train_metadata_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv' if iskaggle else 'data/isic-2024-challenge/train-metadata.csv'\n","test_metadata_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv' if iskaggle else 'data/isic-2024-challenge/test-metadata.csv'\n","\n","train_metadata_df = pd.read_csv(train_metadata_path)\n","test_metadata_df = pd.read_csv(test_metadata_path)\n","\n","print(len(train_metadata_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:02.957130Z","iopub.status.busy":"2024-07-16T13:31:02.956732Z","iopub.status.idle":"2024-07-16T13:31:03.082410Z","shell.execute_reply":"2024-07-16T13:31:03.081270Z","shell.execute_reply.started":"2024-07-16T13:31:02.957097Z"},"trusted":true},"outputs":[],"source":["import sklearn.model_selection as train_test_split\n","\n","train_size = 0.8\n","# Splitting the train dataset into positive and negative samples and saving them in separate dataframes\n","postive_samples = train_metadata_df[train_metadata_df['target'] == 1]\n","negative_samples = train_metadata_df[train_metadata_df['target'] == 0]\n","\n","# TODO: Taking Sample of 1% of the data\n","postive_samples = postive_samples.sample(frac=0.01)\n","negative_samples = negative_samples.sample(frac=0.01)\n","\n","print(f\"Positive samples: {postive_samples.shape}\")\n","print(f\"Negative samples: {negative_samples.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:03.084525Z","iopub.status.busy":"2024-07-16T13:31:03.084056Z","iopub.status.idle":"2024-07-16T13:31:03.603841Z","shell.execute_reply":"2024-07-16T13:31:03.602704Z","shell.execute_reply.started":"2024-07-16T13:31:03.084484Z"},"trusted":true},"outputs":[],"source":["\n","# Splitting each type of samples into train and validation sets\n","train_positive_samples, val_positive_samples = train_test_split.train_test_split(postive_samples,test_size=1-train_size)\n","train_negative_samples, val_negative_samples = train_test_split.train_test_split(negative_samples,test_size=1-train_size)\n","print(f\"Train positive samples: {train_positive_samples.shape}\")\n","print(f\"Train negative samples: {train_negative_samples.shape}\")\n","print(f\"Val positive samples: {val_positive_samples.shape}\")\n","print(f\"Val negative samples: {val_negative_samples.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:03.744484Z","iopub.status.busy":"2024-07-16T13:31:03.744096Z","iopub.status.idle":"2024-07-16T13:31:03.918738Z","shell.execute_reply":"2024-07-16T13:31:03.917693Z","shell.execute_reply.started":"2024-07-16T13:31:03.744452Z"},"trusted":true},"outputs":[],"source":["\n","# Concatenating the positive and negative samples to get the train and validation sets\n","train_metadata_df = pd.concat([train_positive_samples, train_negative_samples])\n","val_metadata_df = pd.concat([val_positive_samples, val_negative_samples])\n","print(f\"Train samples: {train_metadata_df.shape}\")\n","print(f\"Val samples: {val_metadata_df.shape}\")\n","print(f\"Test samples: {test_metadata_df.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:04.446395Z","iopub.status.busy":"2024-07-16T13:31:04.445471Z","iopub.status.idle":"2024-07-16T13:31:04.451408Z","shell.execute_reply":"2024-07-16T13:31:04.450301Z","shell.execute_reply.started":"2024-07-16T13:31:04.446362Z"},"trusted":true},"outputs":[],"source":["import h5py\n","from io import BytesIO\n","\n","train_hdf5_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5' if iskaggle else 'data/isic-2024-challenge/train-image.hdf5'\n","test_hdf5_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5' if iskaggle else 'data/isic-2024-challenge/test-image.hdf5'\n","train_image_path = '/kaggle/input/isic-2024-challenge/train-image/image' if iskaggle else 'data/isic-2024-challenge/train-image/image'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:05.522744Z","iopub.status.busy":"2024-07-16T13:31:05.521693Z","iopub.status.idle":"2024-07-16T13:31:05.532479Z","shell.execute_reply":"2024-07-16T13:31:05.531377Z","shell.execute_reply.started":"2024-07-16T13:31:05.522696Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, hdf5_file_path, metadata_df,target=None, transform=None):\n","        self.hdf5_file = None\n","        self.hdf5_file_path = hdf5_file_path\n","        self.metadata_df = metadata_df\n","        self.image_ids = metadata_df['isic_id']\n","        self.labels = target\n","        self.transform = transform\n","\n","        self.mean_of_color_channels = None  # Initialize as None\n","        self.std_of_color_channels = None   # Initialize as None\n","        # self._calculate_stats()\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, idx):\n","        image_id = self.image_ids.iloc[idx]\n","        self.hdf5_file = h5py.File(self.hdf5_file_path, 'r')\n","        image = np.array(Image.open(BytesIO(self.hdf5_file[image_id][()])),dtype=np.float32)/255\n","        \n","        if self.transform:\n","            image = self.transform(image=image)\n","            image = image['image']\n","\n","        if self.labels is not None:\n","            label = self.labels.iloc[idx]\n","            return image, label\n","        else:\n","            return image\n","\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:06.318837Z","iopub.status.busy":"2024-07-16T13:31:06.318471Z","iopub.status.idle":"2024-07-16T13:31:06.324011Z","shell.execute_reply":"2024-07-16T13:31:06.322961Z","shell.execute_reply.started":"2024-07-16T13:31:06.318808Z"},"trusted":true},"outputs":[],"source":["# HyperParameters\n","dim = 50 \n","batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:06.941539Z","iopub.status.busy":"2024-07-16T13:31:06.940144Z","iopub.status.idle":"2024-07-16T13:31:06.950280Z","shell.execute_reply":"2024-07-16T13:31:06.949190Z","shell.execute_reply.started":"2024-07-16T13:31:06.941489Z"},"trusted":true},"outputs":[],"source":["train_transform = A.Compose([\n","    A.Resize(height=dim, width=dim), #resize \n","    A.OneOf([A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15),\n","             A.RandomBrightnessContrast() \n","             ], p=0.5),\n","    A.HorizontalFlip(p=0.5),\n","    A.VerticalFlip(p=0.5),\n","    # !!One needs to change the mean and std values to appropriate ones for this dataset.!!\n","    A.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1.0),\n","    ToTensorV2(),\n","])\n","\n","\n","test_transform = A.Compose([\n","    A.Resize(height=dim, width=dim), #resize \n","    # !!One needs to change the mean and std values to appropriate ones for this dataset.!!\n","    A.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1.0),\n","    ToTensorV2(),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:07.568232Z","iopub.status.busy":"2024-07-16T13:31:07.567521Z","iopub.status.idle":"2024-07-16T13:31:07.574342Z","shell.execute_reply":"2024-07-16T13:31:07.573165Z","shell.execute_reply.started":"2024-07-16T13:31:07.568188Z"},"trusted":true},"outputs":[],"source":["train_dataset = CustomDataset(train_hdf5_path,train_metadata_df,target=train_metadata_df['target'],transform=train_transform)\n","# train_image_dataset = CustomDatasetImage(train_image_path,train_metadata_df,target=train_metadata_df['target'],transform=train_transform)\n","val_dataset = CustomDataset(train_hdf5_path,val_metadata_df,target=val_metadata_df['target'],transform=train_transform)\n","test_dataset = CustomDataset(test_hdf5_path,test_metadata_df,transform=test_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:31:08.303502Z","iopub.status.busy":"2024-07-16T13:31:08.302559Z","iopub.status.idle":"2024-07-16T13:31:08.309560Z","shell.execute_reply":"2024-07-16T13:31:08.308420Z","shell.execute_reply.started":"2024-07-16T13:31:08.303467Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4,pin_memory=True)\n","val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True,num_workers=4,pin_memory=True)\n","test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:32:32.457341Z","iopub.status.busy":"2024-07-16T13:32:32.456875Z","iopub.status.idle":"2024-07-16T13:32:32.464058Z","shell.execute_reply":"2024-07-16T13:32:32.463119Z","shell.execute_reply.started":"2024-07-16T13:32:32.457305Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_curve, auc\n","\n","def calculate_partial_auc_by_tpr(y_true, y_scores, max_tpr=0.8):\n","    # Calculate the ROC curve\n","    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n","    \n","    # Limit the TPR to the specified maximum\n","    mask = tpr <= max_tpr\n","    fpr, tpr = fpr[mask], tpr[mask]\n","    \n","    # Calculate the partial AUC\n","    partial_auc = auc(fpr, tpr)\n","    \n","    # Normalize the partial AUC to the range [0, 1]\n","    partial_auc /= max_tpr\n","    \n","    return partial_auc"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:32:32.769589Z","iopub.status.busy":"2024-07-16T13:32:32.769205Z","iopub.status.idle":"2024-07-16T13:32:32.808585Z","shell.execute_reply":"2024-07-16T13:32:32.807420Z","shell.execute_reply.started":"2024-07-16T13:32:32.769560Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self,num_classes,device,dim = 32,num_epochs = 20,learning_rate = 0.001,early_stopping = False):\n","        super().__init__()\n","        self.num_of_classes = num_classes\n","        self.device = device\n","        self.dim = dim\n","        # Debugging\n","        self.DEBUG = True\n","        # Hyperparameters\n","        self.num_epochs = num_epochs\n","        self.learning_rate = learning_rate\n","        self.early_stopping = early_stopping\n","        # History while Training\n","        self.model_loss_history = []\n","        self.model_train_acc_history = []\n","        self.model_val_acc_history = []\n","        self.model_val_precision_history = []\n","        self.model_val_recall_history = []\n","        self.model_val_pauc_history = []\n","        self.model_lr_history = []\n","\n","        # Model Attributes\n","        self.criterion = nn.BCEWithLogitsLoss()\n","        self.optimizer = None\n","        self.accuracy = Accuracy(task= 'binary', average='macro').to(self.device)\n","        self.precision = Precision(task= 'binary', average='macro').to(self.device)\n","        self.recall = Recall(task= 'binary', average='macro').to(self.device)\n","        # Feature Extraction\n","        self.feature_extract = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Flatten()\n","        )\n","        \n","        # Classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(4608, 512),  # Adjust the size based on the input image size\n","            nn.ReLU(),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(512, 1),\n","        )\n","        \n","    def forward(self, x):\n","        x = self.feature_extract(x)\n","        x = self.classifier(x)\n","        return x\n","    \n","    def predict(self, img):\n","        '''\n","        returns the predicted classes for the given images\n","        '''\n","        self.eval()\n","        with torch.no_grad():\n","            img = img.to(self.device)\n","            output = self(img)\n","            _, predicted = torch.max(output, 1)\n","            return predicted\n","        \n","\n","    \n","    def eval_val(self, data_loader):\n","        '''\n","        returns accuracy, precision and recall\n","        '''\n","        self.eval()\n","        y_pred = []\n","        y_actual = []\n","        with torch.no_grad():\n","            for images, labels in data_loader:\n","                \n","                images, labels = images.to(self.device), labels.to(self.device)\n","                outputs = self(images)\n","                labels = labels.unsqueeze(1).float()\n","                self.accuracy(outputs, labels)\n","                self.precision(outputs, labels)\n","                self.recall(outputs, labels)\n","\n","                y_pred.extend(outputs.cpu().numpy())\n","                y_actual.extend(labels.cpu().numpy())\n","        \n","        partial_auc = calculate_partial_auc_by_tpr(y_actual, y_pred)\n","\n","\n","        return self.accuracy.compute(), self.precision.compute(), self.recall.compute(), partial_auc, y_pred, y_actual\n","    \n","    def train_model(self, train_loader, val_loader):\n","        \n","        last_accuracy = -100\n","        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n","        scaler = GradScaler()\n","\n","        for epoch in range(self.num_epochs):\n","            self.train()\n","            running_loss = 0.0\n","\n","            for i, (images, labels) in enumerate(train_loader):\n","                \n","                images, labels = images.to(self.device), labels.to(self.device)\n","                self.optimizer.zero_grad()\n","                with autocast():\n","                    outputs = self(images)\n","                    labels = labels.unsqueeze(1).float()\n","                    loss = self.criterion(outputs, labels)\n","                scaler.scale(loss).backward()\n","                scaler.step(self.optimizer)\n","                scaler.update()\n","\n","                running_loss += loss.item()\n","                if i%1000 == 0 and self.DEBUG:\n","                    print(\" Step [{}/{}] Loss: {}\".format(i, len(train_loader), loss.item()))\n","                    \n","            val_acc, val_precision, val_recall, val_pauc, _ , _ = self.eval_val(val_loader)\n","            train_acc, _, _, _, _, _ = self.eval_val(train_loader)\n","\n","            self.model_loss_history.append(running_loss/len(train_loader))\n","            self.model_train_acc_history.append(train_acc.item())\n","            self.model_val_acc_history.append(val_acc.item())\n","            self.model_val_precision_history.append(val_precision.item())\n","            self.model_val_recall_history.append(val_recall.item())\n","            self.model_val_pauc_history.append(val_pauc)\n","            self.model_lr_history.append(self.optimizer.param_groups[0]['lr'])\n","            \n","            print(f'Epoch: {epoch+1}/{self.num_epochs}, Loss: {loss.item()},Train Acc: {train_acc}, Val Acc: {val_acc}, Val Precision: {val_precision}, Val Recall: {val_recall}, Val PAUC: {val_pauc}')\n","            \n","            if val_acc > last_accuracy:\n","                last_accuracy = val_acc\n","            elif self.early_stopping:\n","                break\n","        \n","        print('Finished Training')\n","\n","    def plot_history(self):\n","        # making two plots one for loss and other for accuracy\n","        fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n","        fig.suptitle('Model Training History')\n","        axs[0, 0].plot(self.model_loss_history)\n","        axs[0, 0].set_title('Model Loss')\n","        axs[0, 0].set_xlabel('Epochs')\n","        axs[0, 0].set_ylabel('Loss')\n","\n","        axs[0, 1].plot(self.model_train_acc_history, label='Train')\n","        axs[0, 1].plot(self.model_val_acc_history, label='Val')\n","        axs[0, 1].set_title('Model Accuracy')\n","        axs[0, 1].set_xlabel('Epochs')\n","        axs[0, 1].set_ylabel('Accuracy')\n","        axs[0, 1].legend()\n","\n","        axs[1, 0].plot(self.model_val_precision_history)\n","        axs[1, 0].set_title('Model Precision')\n","        axs[1, 0].set_xlabel('Epochs')\n","        axs[1, 0].set_ylabel('Precision')\n","        \n","        axs[1, 1].plot(self.model_val_recall_history)\n","        axs[1, 1].set_title('Model Recall')\n","        axs[1, 1].set_xlabel('Epochs')\n","        axs[1, 1].set_ylabel('Recall')\n","\n","        axs[0, 2].plot(self.model_lr_history)\n","        axs[0, 2].set_title('Learning Rate')\n","        axs[0, 2].set_xlabel('Epochs')\n","        axs[0, 2].set_ylabel('Learning Rate')\n","        \n","        axs[1, 2].plot(self.model_val_pauc_history)\n","        axs[1, 2].set_title('Model Partial AUC')\n","        axs[1, 2].set_xlabel('Epochs')\n","        axs[1, 2].set_ylabel('Partial AUC')\n","        \n","\n","        plt.show()\n","    \n","    def save_model(self):\n","        torch.save(self.state_dict(),type(self).__name__+'.pth')\n","\n","    def print_summary(self):\n","        summary(self, (3, self.dim, self.dim))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:32:33.420230Z","iopub.status.busy":"2024-07-16T13:32:33.419490Z","iopub.status.idle":"2024-07-16T13:32:33.462961Z","shell.execute_reply":"2024-07-16T13:32:33.462072Z","shell.execute_reply.started":"2024-07-16T13:32:33.420184Z"},"trusted":true},"outputs":[],"source":["num_of_classes = 2\n","cnn = Model(num_classes=num_of_classes, \n","            device=device, \n","            dim=dim, \n","            num_epochs=2, \n","            learning_rate=0.001,\n","            early_stopping=False)\n","cnn.to(device)\n","cnn.print_summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-16T13:32:34.699465Z","iopub.status.busy":"2024-07-16T13:32:34.698558Z"},"trusted":true},"outputs":[],"source":["cnn.train_model(train_loader=train_loader,val_loader=val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T15:42:14.705953Z","iopub.status.busy":"2024-07-14T15:42:14.705636Z","iopub.status.idle":"2024-07-14T15:42:53.485866Z","shell.execute_reply":"2024-07-14T15:42:53.484740Z","shell.execute_reply.started":"2024-07-14T15:42:14.705922Z"},"trusted":true},"outputs":[],"source":["cnn_acc , cnn_precision, cnn_recall , cnn_pauc, y_pred, y_actual = cnn.eval_val(val_loader)\n","print(f\"Accuracy: {cnn_acc}, Precision: {cnn_precision}, Recall: {cnn_recall}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T15:48:07.801716Z","iopub.status.busy":"2024-07-14T15:48:07.801272Z","iopub.status.idle":"2024-07-14T15:48:07.807550Z","shell.execute_reply":"2024-07-14T15:48:07.806491Z","shell.execute_reply.started":"2024-07-14T15:48:07.801685Z"},"trusted":true},"outputs":[],"source":["print(f\"Partial AUC: {cnn_pauc}\")\n","print(y_actual[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-14T15:46:18.479979Z","iopub.status.busy":"2024-07-14T15:46:18.479675Z","iopub.status.idle":"2024-07-14T15:46:19.937873Z","shell.execute_reply":"2024-07-14T15:46:19.936442Z","shell.execute_reply.started":"2024-07-14T15:46:18.479954Z"},"trusted":true},"outputs":[],"source":["cnn.plot_history()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cnn.save_model()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9094797,"sourceId":63056,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
