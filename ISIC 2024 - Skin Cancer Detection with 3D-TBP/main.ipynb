{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:30:46.769468Z","iopub.execute_input":"2024-07-23T09:30:46.770105Z","iopub.status.idle":"2024-07-23T09:31:00.095510Z","shell.execute_reply.started":"2024-07-23T09:30:46.770055Z","shell.execute_reply":"2024-07-23T09:31:00.094401Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.torch_version\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchsummary import summary\nfrom torchvision import transforms\nfrom torchmetrics import Accuracy, Precision, Recall\n\nimport cv2\nfrom joblib import Parallel, delayed\n\nfrom torch.cuda.amp import GradScaler, autocast\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:05.472086Z","iopub.execute_input":"2024-07-23T09:31:05.472989Z","iopub.status.idle":"2024-07-23T09:31:13.363122Z","shell.execute_reply.started":"2024-07-23T09:31:05.472949Z","shell.execute_reply":"2024-07-23T09:31:13.362349Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:13.364493Z","iopub.execute_input":"2024-07-23T09:31:13.364913Z","iopub.status.idle":"2024-07-23T09:31:13.369513Z","shell.execute_reply.started":"2024-07-23T09:31:13.364887Z","shell.execute_reply":"2024-07-23T09:31:13.368619Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:20.552454Z","iopub.execute_input":"2024-07-23T09:31:20.552810Z","iopub.status.idle":"2024-07-23T09:31:20.587134Z","shell.execute_reply.started":"2024-07-23T09:31:20.552780Z","shell.execute_reply":"2024-07-23T09:31:20.586198Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":"iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nmain_path = '/kaggle/input/isic-2024-challenge' if iskaggle else 'data/isic-2024-challenge'","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:21.545707Z","iopub.execute_input":"2024-07-23T09:31:21.546380Z","iopub.status.idle":"2024-07-23T09:31:21.550537Z","shell.execute_reply.started":"2024-07-23T09:31:21.546337Z","shell.execute_reply":"2024-07-23T09:31:21.549554Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_metadata_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv' if iskaggle else 'data/isic-2024-challenge/train-metadata.csv'\ntest_metadata_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv' if iskaggle else 'data/isic-2024-challenge/test-metadata.csv'\n\ntrain_metadata_df = pd.read_csv(train_metadata_path)\ntest_metadata_df = pd.read_csv(test_metadata_path)\n\nprint(len(train_metadata_df))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:26.509688Z","iopub.execute_input":"2024-07-23T09:31:26.510758Z","iopub.status.idle":"2024-07-23T09:31:34.007894Z","shell.execute_reply.started":"2024-07-23T09:31:26.510721Z","shell.execute_reply":"2024-07-23T09:31:34.006941Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3275392473.py:4: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_metadata_df = pd.read_csv(train_metadata_path)\n","output_type":"stream"},{"name":"stdout","text":"401059\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn.model_selection as train_test_split\n\ntrain_size = 0.8\n# Splitting the train dataset into positive and negative samples and saving them in separate dataframes\npostive_samples = train_metadata_df[train_metadata_df['target'] == 1]\nnegative_samples = train_metadata_df[train_metadata_df['target'] == 0]\n\n# TODO: Taking Sample of 1% of the data\n# postive_samples = postive_samples.sample(frac=0.1)\nnegative_samples = negative_samples.sample(frac=5*postive_samples.shape[0]/negative_samples.shape[0])\n\nprint(f\"Positive samples: {postive_samples.shape}\")\nprint(f\"Negative samples: {negative_samples.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:36.053180Z","iopub.execute_input":"2024-07-23T09:31:36.054016Z","iopub.status.idle":"2024-07-23T09:31:36.238256Z","shell.execute_reply.started":"2024-07-23T09:31:36.053974Z","shell.execute_reply":"2024-07-23T09:31:36.237126Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Positive samples: (393, 55)\nNegative samples: (1965, 55)\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Splitting each type of samples into train and validation sets\ntrain_positive_samples, val_positive_samples = train_test_split.train_test_split(postive_samples,test_size=1-train_size)\ntrain_negative_samples, val_negative_samples = train_test_split.train_test_split(negative_samples,test_size=1-train_size)\nprint(f\"Train positive samples: {train_positive_samples.shape}\")\nprint(f\"Train negative samples: {train_negative_samples.shape}\")\nprint(f\"Val positive samples: {val_positive_samples.shape}\")\nprint(f\"Val negative samples: {val_negative_samples.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:37.520785Z","iopub.execute_input":"2024-07-23T09:31:37.521416Z","iopub.status.idle":"2024-07-23T09:31:37.531411Z","shell.execute_reply.started":"2024-07-23T09:31:37.521381Z","shell.execute_reply":"2024-07-23T09:31:37.530421Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Train positive samples: (314, 55)\nTrain negative samples: (1572, 55)\nVal positive samples: (79, 55)\nVal negative samples: (393, 55)\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Concatenating the positive and negative samples to get the train and validation sets\ntrain_metadata_df = pd.concat([train_positive_samples, train_negative_samples])\nval_metadata_df = pd.concat([val_positive_samples, val_negative_samples])\nprint(f\"Train samples: {train_metadata_df.shape}\")\nprint(f\"Val samples: {val_metadata_df.shape}\")\nprint(f\"Test samples: {test_metadata_df.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:38.766756Z","iopub.execute_input":"2024-07-23T09:31:38.767648Z","iopub.status.idle":"2024-07-23T09:31:38.820141Z","shell.execute_reply.started":"2024-07-23T09:31:38.767610Z","shell.execute_reply":"2024-07-23T09:31:38.819280Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Train samples: (1886, 55)\nVal samples: (472, 55)\nTest samples: (3, 44)\n","output_type":"stream"}]},{"cell_type":"code","source":"import h5py\nfrom io import BytesIO\n\ntrain_hdf5_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5' if iskaggle else 'data/isic-2024-challenge/train-image.hdf5'\ntest_hdf5_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5' if iskaggle else 'data/isic-2024-challenge/test-image.hdf5'\ntrain_image_path = '/kaggle/input/isic-2024-challenge/train-image/image' if iskaggle else 'data/isic-2024-challenge/train-image/image'","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:53.624310Z","iopub.execute_input":"2024-07-23T09:31:53.625013Z","iopub.status.idle":"2024-07-23T09:31:53.755990Z","shell.execute_reply.started":"2024-07-23T09:31:53.624977Z","shell.execute_reply":"2024-07-23T09:31:53.755065Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, hdf5_file_path, metadata_df,target=None, transform=None):\n        self.hdf5_file_path = hdf5_file_path\n        self.hdf5_file = h5py.File(self.hdf5_file_path, 'r')\n        self.metadata_df = metadata_df\n        self.image_ids = metadata_df['isic_id']\n        self.labels = target\n        self.transform = transform\n\n        self.mean_of_color_channels = None  # Initialize as None\n        self.std_of_color_channels = None   # Initialize as None\n        # self._calculate_stats()\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids.iloc[idx]\n        image = np.array(Image.open(BytesIO(np.array(self.hdf5_file[image_id]))),dtype=np.float32)/255\n        \n        if self.transform:\n            image = self.transform(image=image)\n            image = image['image']\n\n        if self.labels is not None:\n            label = self.labels.iloc[idx]\n            return image, label\n        else:\n            return image\n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:31:54.820612Z","iopub.execute_input":"2024-07-23T09:31:54.820980Z","iopub.status.idle":"2024-07-23T09:31:54.830188Z","shell.execute_reply.started":"2024-07-23T09:31:54.820946Z","shell.execute_reply":"2024-07-23T09:31:54.829195Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# HyperParameters\ndim = 224 \nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:32:16.589702Z","iopub.execute_input":"2024-07-23T09:32:16.590084Z","iopub.status.idle":"2024-07-23T09:32:16.594425Z","shell.execute_reply.started":"2024-07-23T09:32:16.590050Z","shell.execute_reply":"2024-07-23T09:32:16.593416Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_transform = A.Compose([\n    A.Resize(height=dim, width=dim), #resize \n    A.OneOf([A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15),\n             A.RandomBrightnessContrast() \n             ], p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    # !!One needs to change the mean and std values to appropriate ones for this dataset.!!\n    A.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1.0),\n    ToTensorV2(),\n])\n\n\ntest_transform = A.Compose([\n    A.Resize(height=dim, width=dim), #resize \n    # !!One needs to change the mean and std values to appropriate ones for this dataset.!!\n    A.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1.0),\n    ToTensorV2(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:32:19.267488Z","iopub.execute_input":"2024-07-23T09:32:19.267883Z","iopub.status.idle":"2024-07-23T09:32:19.275840Z","shell.execute_reply.started":"2024-07-23T09:32:19.267851Z","shell.execute_reply":"2024-07-23T09:32:19.274930Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_hdf5_path,train_metadata_df,target=train_metadata_df['target'],transform=train_transform)\n# train_image_dataset = CustomDatasetImage(train_image_path,train_metadata_df,target=train_metadata_df['target'],transform=train_transform)\nval_dataset = CustomDataset(train_hdf5_path,val_metadata_df,target=val_metadata_df['target'],transform=train_transform)\ntest_dataset = CustomDataset(test_hdf5_path,test_metadata_df,transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:32:21.857084Z","iopub.execute_input":"2024-07-23T09:32:21.857802Z","iopub.status.idle":"2024-07-23T09:32:21.873399Z","shell.execute_reply.started":"2024-07-23T09:32:21.857768Z","shell.execute_reply":"2024-07-23T09:32:21.872648Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"n_w = 0\nif iskaggle:\n    n_w = 4\ntrain_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=n_w)\nval_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True,num_workers=n_w)\ntest_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=n_w)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:32:23.014313Z","iopub.execute_input":"2024-07-23T09:32:23.014688Z","iopub.status.idle":"2024-07-23T09:32:23.020483Z","shell.execute_reply.started":"2024-07-23T09:32:23.014656Z","shell.execute_reply":"2024-07-23T09:32:23.019556Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ndef calculate_partial_auc_by_tpr(y_true, y_scores, max_tpr=0.8):\n    # Calculate the ROC curve\n    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n    \n    # Limit the TPR to the specified minimum TPR\n    mask = tpr >= max_tpr\n    fpr, tpr = fpr[mask], tpr[mask]\n    \n    # Calculate the partial AUC\n    partial_auc = auc(fpr, tpr)\n    \n    # Normalize the partial AUC to the range [0, 1]\n    partial_auc /= max_tpr\n    \n    return partial_auc","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:32:25.196155Z","iopub.execute_input":"2024-07-23T09:32:25.196868Z","iopub.status.idle":"2024-07-23T09:32:25.202407Z","shell.execute_reply.started":"2024-07-23T09:32:25.196832Z","shell.execute_reply":"2024-07-23T09:32:25.201397Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,num_classes,device,dim = 32,num_epochs = 20,learning_rate = 0.001,early_stopping = False):\n        super().__init__()\n        self.num_of_classes = num_classes\n        self.device = device\n        self.dim = dim\n        # Debugging\n        self.DEBUG = True\n        # Hyperparameters\n        self.num_epochs = num_epochs\n        self.learning_rate = learning_rate\n        self.early_stopping = early_stopping\n        # History while Training\n        self.model_loss_history = []\n\n        self.model_train_acc_history = []\n        self.model_train_pauc_history = []\n        \n        self.model_val_acc_history = []\n        self.model_val_precision_history = []\n        self.model_val_recall_history = []\n        self.model_val_pauc_history = []\n        \n        self.model_lr_history = []\n\n        # Model Attributes\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.optimizer = None\n        self.accuracy = Accuracy(task= 'binary', average='macro').to(self.device)\n        self.precision = Precision(task= 'binary', average='macro').to(self.device)\n        self.recall = Recall(task= 'binary', average='macro').to(self.device)\n        # Feature Extraction\n        self.feature_extract = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Flatten()\n        )\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(1152 , 200),  # Adjust the size based on the input image size\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(200, 100),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(100, 1)\n\n        )\n        \n    def forward(self, x):\n        x = self.feature_extract(x)\n        x = self.classifier(x)\n        return x\n    \n    def predict(self, img):\n        '''\n        returns the predicted classes for the given images\n        '''\n        self.eval()\n        with torch.no_grad():\n            img = img.to(self.device)\n            output = self(img)\n            prob_outputs = torch.sigmoid(output)\n            return prob_outputs\n            \n        \n\n    \n    def eval_val(self, data_loader):\n        '''\n        returns the accuracy, precision, recall , partial AUC, predicted probabilities and actual labels for the given data loader\n        '''\n        self.eval()\n        y_prob_pred = []\n        y_actual = []\n        with torch.no_grad():\n            for images, actuals in data_loader:\n                \n                images, actuals = images.to(self.device), actuals.to(self.device)\n                # Get the model predictions\n                output = self(images)\n                # Get the probability outputs\n                prob_outputs = torch.sigmoid(output)\n                # Get the binary predictions\n                batch_pred = (prob_outputs > 0.5).float()\n                # Reshape the actuals\n                actuals = actuals.unsqueeze(1).float()\n\n                y_prob_pred.extend(prob_outputs.cpu().numpy())\n                y_actual.extend(actuals.cpu().numpy())\n\n                # Update the metrics\n                self.accuracy(batch_pred, actuals)\n                self.precision(batch_pred, actuals)\n                self.recall(batch_pred, actuals)\n        \n        # Calculate the partial AUC\n        partial_auc = calculate_partial_auc_by_tpr(y_true=y_actual, y_scores=y_prob_pred, max_tpr=0.8)\n\n\n        return self.accuracy.compute(), self.precision.compute(), self.recall.compute(), partial_auc, y_prob_pred, y_actual\n    \n    def train_model(self, train_loader, val_loader):\n        \n        last_accuracy = -100\n        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        scaler = GradScaler()\n\n        for epoch in range(self.num_epochs):\n            self.train()\n            running_loss = 0.0\n\n            for i, (images, labels) in enumerate(train_loader):\n                \n                images, labels = images.to(self.device), labels.to(self.device)\n                self.optimizer.zero_grad()\n                with autocast():\n                    outputs = self(images)\n                    labels = labels.unsqueeze(1).float()\n                    loss = self.criterion(outputs, labels)\n                scaler.scale(loss).backward()\n                scaler.step(self.optimizer)\n                scaler.update()\n\n                running_loss += loss.item()\n                if i%1000 == 0 and self.DEBUG:\n                    print(\" Step [{}/{}] Loss: {}\".format(i, len(train_loader), loss.item()))\n                    \n            val_acc, val_precision, val_recall, val_pauc, _ , _ = self.eval_val(val_loader)\n            train_acc, _, _, train_pauc, _, _ = self.eval_val(train_loader)\n\n            self.model_loss_history.append(running_loss/len(train_loader))\n            self.model_train_acc_history.append(train_acc.item())\n            self.model_train_pauc_history.append(train_pauc)\n            self.model_val_acc_history.append(val_acc.item())\n            self.model_val_precision_history.append(val_precision.item())\n            self.model_val_recall_history.append(val_recall.item())\n            self.model_val_pauc_history.append(val_pauc)\n            self.model_lr_history.append(self.optimizer.param_groups[0]['lr'])\n            \n            print(f'Epoch: {epoch+1}/{self.num_epochs}, Loss: {loss.item()},Train Acc: {train_acc}, Val Acc: {val_acc}, Val Precision: {val_precision}, Val Recall: {val_recall},Train PAUC: {train_pauc}, Val PAUC: {val_pauc}')\n            \n            if val_acc > last_accuracy:\n                last_accuracy = val_acc\n            elif self.early_stopping:\n                break\n        \n        print('Finished Training')\n\n    def plot_history(self):\n        # making two plots one for loss and other for accuracy\n        fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n        fig.suptitle('Model Training History')\n        axs[0, 0].plot(self.model_loss_history)\n        axs[0, 0].set_title('Model Loss')\n        axs[0, 0].set_xlabel('Epochs')\n        axs[0, 0].set_ylabel('Loss')\n\n        axs[0, 1].plot(self.model_train_acc_history, label='Train')\n        axs[0, 1].plot(self.model_val_acc_history, label='Val')\n        axs[0, 1].set_title('Model Accuracy')\n        axs[0, 1].set_xlabel('Epochs')\n        axs[0, 1].set_ylabel('Accuracy')\n        axs[0, 1].legend()\n\n        axs[1, 0].plot(self.model_val_precision_history)\n        axs[1, 0].set_title('Model Precision')\n        axs[1, 0].set_xlabel('Epochs')\n        axs[1, 0].set_ylabel('Precision')\n        \n        axs[1, 1].plot(self.model_val_recall_history)\n        axs[1, 1].set_title('Model Recall')\n        axs[1, 1].set_xlabel('Epochs')\n        axs[1, 1].set_ylabel('Recall')\n\n        axs[0, 2].plot(self.model_lr_history)\n        axs[0, 2].set_title('Learning Rate')\n        axs[0, 2].set_xlabel('Epochs')\n        axs[0, 2].set_ylabel('Learning Rate')\n        \n        axs[1, 2].plot(self.model_val_pauc_history)\n        axs[1, 2].set_title('Model Partial AUC')\n        axs[1, 2].set_xlabel('Epochs')\n        axs[1, 2].set_ylabel('Partial AUC')\n        \n\n        plt.show()\n    \n    def save_model(self):\n        torch.save(self.state_dict(),type(self).__name__+'.pth')\n\n    def print_summary(self):\n        summary(self, (3, self.dim, self.dim))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:32:27.154191Z","iopub.execute_input":"2024-07-23T09:32:27.154649Z","iopub.status.idle":"2024-07-23T09:32:27.190937Z","shell.execute_reply.started":"2024-07-23T09:32:27.154612Z","shell.execute_reply":"2024-07-23T09:32:27.189892Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"num_of_classes = 2\ncnn = Model(num_classes=num_of_classes, \n            device=device, \n            dim=dim, \n            num_epochs=20, \n            learning_rate=0.001,\n            early_stopping=False)\ncnn.to(device)\ncnn.print_summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:33:47.682136Z","iopub.execute_input":"2024-07-23T09:33:47.682444Z","iopub.status.idle":"2024-07-23T09:33:47.709638Z","shell.execute_reply.started":"2024-07-23T09:33:47.682420Z","shell.execute_reply":"2024-07-23T09:33:47.708740Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 220, 220]           4,736\n         MaxPool2d-2         [-1, 32, 110, 110]               0\n            Conv2d-3         [-1, 64, 108, 108]          51,264\n         MaxPool2d-4           [-1, 64, 54, 54]               0\n            Conv2d-5          [-1, 128, 52, 52]         204,928\n         MaxPool2d-6          [-1, 128, 26, 26]               0\n            Conv2d-7           [-1, 64, 26, 26]          73,792\n         MaxPool2d-8           [-1, 64, 13, 13]               0\n            Conv2d-9           [-1, 32, 13, 13]          18,464\n        MaxPool2d-10             [-1, 32, 6, 6]               0\n          Flatten-11                 [-1, 1152]               0\n           Linear-12                  [-1, 200]         230,600\n             ReLU-13                  [-1, 200]               0\n          Dropout-14                  [-1, 200]               0\n           Linear-15                  [-1, 100]          20,100\n             ReLU-16                  [-1, 100]               0\n          Dropout-17                  [-1, 100]               0\n           Linear-18                    [-1, 1]             101\n================================================================\nTotal params: 603,985\nTrainable params: 603,985\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 25.67\nParams size (MB): 2.30\nEstimated Total Size (MB): 28.55\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"cnn.train_model(train_loader=train_loader,val_loader=val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:33:47.712297Z","iopub.execute_input":"2024-07-23T09:33:47.712566Z","iopub.status.idle":"2024-07-23T09:36:08.448574Z","shell.execute_reply.started":"2024-07-23T09:33:47.712542Z","shell.execute_reply":"2024-07-23T09:36:08.447462Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":" Step [0/30] Loss: 0.6875759363174438\nEpoch: 1/20, Loss: 0.3850792348384857,Train Acc: 0.8333333134651184, Val Acc: 0.8326271176338196, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.25535191893162185, Val PAUC: 0.24988324153702446\n Step [0/30] Loss: 0.6708428859710693\nEpoch: 2/20, Loss: 0.5921714901924133,Train Acc: 0.8333333134651184, Val Acc: 0.8332155346870422, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.25630915625354533, Val PAUC: 0.3329025348664927\n Step [0/30] Loss: 0.6125015616416931\nEpoch: 3/20, Loss: 0.47450926899909973,Train Acc: 0.8333333134651184, Val Acc: 0.8332690596580505, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.2705752844362328, Val PAUC: 0.43609285921345053\n Step [0/30] Loss: 0.4267982840538025\nEpoch: 4/20, Loss: 0.6242520809173584,Train Acc: 0.8333333134651184, Val Acc: 0.8332891464233398, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.22628659786713337, Val PAUC: 0.3856250201307694\n Step [0/30] Loss: 0.4942363202571869\nEpoch: 5/20, Loss: 0.5490688681602478,Train Acc: 0.8333333134651184, Val Acc: 0.8332996964454651, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.33910765222605793, Val PAUC: 0.35601265822784806\n Step [0/30] Loss: 0.4500414729118347\nEpoch: 6/20, Loss: 0.4614872336387634,Train Acc: 0.8333333134651184, Val Acc: 0.8333061337471008, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.28720538362425246, Val PAUC: 0.2558419493026702\n Step [0/30] Loss: 0.43031686544418335\nEpoch: 7/20, Loss: 0.42286941409111023,Train Acc: 0.8333333134651184, Val Acc: 0.8333105444908142, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.2982933826031993, Val PAUC: 0.25310416465358965\n Step [0/30] Loss: 0.5029218196868896\nEpoch: 8/20, Loss: 0.46081212162971497,Train Acc: 0.8333333134651184, Val Acc: 0.8333137035369873, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.29653085039140376, Val PAUC: 0.3460479273359744\n Step [0/30] Loss: 0.534575343132019\nEpoch: 9/20, Loss: 0.6721215844154358,Train Acc: 0.8333333134651184, Val Acc: 0.8333160877227783, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.22213856947213173, Val PAUC: 0.181519148387928\n Step [0/30] Loss: 0.49722573161125183\nEpoch: 10/20, Loss: 0.23989005386829376,Train Acc: 0.8333333134651184, Val Acc: 0.8333179950714111, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.2407692440154941, Val PAUC: 0.2638942570940831\n Step [0/30] Loss: 0.41336339712142944\nEpoch: 11/20, Loss: 0.5170032382011414,Train Acc: 0.8333333134651184, Val Acc: 0.8333194851875305, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.3496448598888186, Val PAUC: 0.1246899861500306\n Step [0/30] Loss: 0.3935202360153198\nEpoch: 12/20, Loss: 0.7006405591964722,Train Acc: 0.8333333134651184, Val Acc: 0.8333207368850708, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.30742638895641883, Val PAUC: 0.3937578510000965\n Step [0/30] Loss: 0.44385403394699097\nEpoch: 13/20, Loss: 0.5111037492752075,Train Acc: 0.8333333134651184, Val Acc: 0.833321750164032, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.36357291616019194, Val PAUC: 0.22939011820787839\n Step [0/30] Loss: 0.3769908547401428\nEpoch: 14/20, Loss: 0.5993538498878479,Train Acc: 0.8333333134651184, Val Acc: 0.8333226442337036, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.16985771097713162, Val PAUC: 0.24100557219699165\n Step [0/30] Loss: 0.3597686290740967\nEpoch: 15/20, Loss: 0.43638306856155396,Train Acc: 0.8333333134651184, Val Acc: 0.8333233594894409, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.342346558402619, Val PAUC: 0.27373820336908555\n Step [0/30] Loss: 0.4564257860183716\nEpoch: 16/20, Loss: 0.3914482891559601,Train Acc: 0.8333333134651184, Val Acc: 0.8333240151405334, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.315352719161764, Val PAUC: 0.19011498695526133\n Step [0/30] Loss: 0.5781639814376831\nEpoch: 17/20, Loss: 0.48192158341407776,Train Acc: 0.8333333134651184, Val Acc: 0.8333246111869812, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.38986022309200824, Val PAUC: 0.22087480271845905\n Step [0/30] Loss: 0.44938552379608154\nEpoch: 18/20, Loss: 0.642393171787262,Train Acc: 0.8333333134651184, Val Acc: 0.8333250880241394, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.28121885180142947, Val PAUC: 0.30635004992430825\n Step [0/30] Loss: 0.4350930452346802\nEpoch: 19/20, Loss: 0.42437630891799927,Train Acc: 0.8333333134651184, Val Acc: 0.8333255648612976, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.43215086667963243, Val PAUC: 0.4775622443392275\n Step [0/30] Loss: 0.4703980088233948\nEpoch: 20/20, Loss: 0.28289076685905457,Train Acc: 0.8333333134651184, Val Acc: 0.833325982093811, Val Precision: 0.0, Val Recall: 0.0,Train PAUC: 0.36407179381209387, Val PAUC: 0.39621380487647756\nFinished Training\n","output_type":"stream"}]},{"cell_type":"code","source":"cnn_acc , cnn_precision, cnn_recall , cnn_pauc, y_pred, y_actual = cnn.eval_val(val_loader)\nprint(f\"Accuracy: {cnn_pauc}, Precision: {cnn_precision}, Recall: {cnn_recall}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:08.450198Z","iopub.execute_input":"2024-07-23T09:36:08.450562Z","iopub.status.idle":"2024-07-23T09:36:09.357998Z","shell.execute_reply.started":"2024-07-23T09:36:08.450531Z","shell.execute_reply":"2024-07-23T09:36:09.356775Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Accuracy: 0.2901850420330467, Precision: 0.0, Recall: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# 0.5 threshold\ny_pred = np.array(y_pred)\ny_actual = np.array(y_actual).astype(int)\ny_pred = (y_pred > 0.5).astype(int)\n\n# output the incorrect predictions\nincorrect_predictions = np.where(y_pred != y_actual)[0]\nnumber_zero = np.sum(y_pred == 0)\nnumber_one = np.sum(y_pred == 1)\nnumber_zero, number_one","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:09.359473Z","iopub.execute_input":"2024-07-23T09:36:09.359813Z","iopub.status.idle":"2024-07-23T09:36:09.370726Z","shell.execute_reply.started":"2024-07-23T09:36:09.359780Z","shell.execute_reply":"2024-07-23T09:36:09.369765Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(472, 0)"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Partial AUC: {cnn_pauc}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:09.371838Z","iopub.execute_input":"2024-07-23T09:36:09.372152Z","iopub.status.idle":"2024-07-23T09:36:09.378927Z","shell.execute_reply.started":"2024-07-23T09:36:09.372128Z","shell.execute_reply":"2024-07-23T09:36:09.378032Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Partial AUC: 0.2901850420330467\n","output_type":"stream"}]},{"cell_type":"code","source":"cnn.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:09.380649Z","iopub.execute_input":"2024-07-23T09:36:09.381005Z","iopub.status.idle":"2024-07-23T09:36:09.396582Z","shell.execute_reply.started":"2024-07-23T09:36:09.380973Z","shell.execute_reply":"2024-07-23T09:36:09.395741Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"y_prod_pred = []\nfor images in test_loader:\n    images = images.to(device)\n    prob_outputs = cnn.predict(images)\n    y_prod_pred.extend(prob_outputs.cpu().numpy())\n\ny_prod_pred = np.array(y_prod_pred)\ny_prod_pred = y_prod_pred.flatten()\ny_prod_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:09.397634Z","iopub.execute_input":"2024-07-23T09:36:09.397909Z","iopub.status.idle":"2024-07-23T09:36:09.556452Z","shell.execute_reply.started":"2024-07-23T09:36:09.397883Z","shell.execute_reply":"2024-07-23T09:36:09.555380Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"array([0.1906663 , 0.15343884, 0.14250077], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'isic_id': test_metadata_df['isic_id'], 'target': y_prod_pred})\nsubmission_df ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:09.557867Z","iopub.execute_input":"2024-07-23T09:36:09.558177Z","iopub.status.idle":"2024-07-23T09:36:09.569446Z","shell.execute_reply.started":"2024-07-23T09:36:09.558148Z","shell.execute_reply":"2024-07-23T09:36:09.568565Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"        isic_id    target\n0  ISIC_0015657  0.190666\n1  ISIC_0015729  0.153439\n2  ISIC_0015740  0.142501","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isic_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0015657</td>\n      <td>0.190666</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0015729</td>\n      <td>0.153439</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0015740</td>\n      <td>0.142501</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T09:36:09.570767Z","iopub.execute_input":"2024-07-23T09:36:09.571130Z","iopub.status.idle":"2024-07-23T09:36:09.580843Z","shell.execute_reply.started":"2024-07-23T09:36:09.571095Z","shell.execute_reply":"2024-07-23T09:36:09.580101Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}